{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL Complaint Facts\n",
    "# If using the native Google BigQuery API module:\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "# import credentials\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow\n",
    "from datetime import datetime\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the GCP Project, dataset and table name\n",
    "gcp_project = 'cis-4400-404715'\n",
    "path_to_service_account_key_file = 'keys.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data( df):\n",
    "    \"\"\"\n",
    "    transform_data\n",
    "    Accepts a data frame\n",
    "    Performs any specific cleaning and transformation steps on the dataframe\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    # Convert the date_of_birth to a datetime64 data type. 2012-08-21 04:12:16.827\n",
    "    df['date_of_birth'] = pd.to_datetime(df['date_of_birth'], format='%m/%d/%Y')\n",
    "    # Convert the postal code into a string\n",
    "    df['incident_zip'] =  df['incident_zip'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
    "    \"\"\"\n",
    "    upload_bigquery_table\n",
    "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
    "    Loads the data into the BigQuery table from the dataframe.\n",
    "    for credentials.\n",
    "    The write disposition is either\n",
    "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
    "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "        \n",
    "        # Submit the job\n",
    "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
    "        \n",
    "        # Show the job results\n",
    "        job.result()\n",
    "    except Exception as err:\n",
    "        print(\"Failed to load BigQuery Table.\", err)\n",
    "        # os._exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_table_exists(table_path, bqclient):\n",
    "    \"\"\"\n",
    "    bigquery_table_exists\n",
    "    Accepts a path to a BigQuery table\n",
    "    Checks if the BigQuery table exists.\n",
    "    Returns True or False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bqclient.get_table(table_path)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        # print(\"Table {} is not found.\".format(table_id))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_table(bqclient, table_path, df):\n",
    "    \"\"\"\n",
    "    build_new_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Add the surrogate key and a record timestamp to the data frame\n",
    "    Inserts the contents of the dataframe to the dimensional table.\n",
    "    \"\"\"\n",
    "    upload_bigquery_table(bqclient, table_path, \"WRITE_TRUNCATE\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_existing_table( bqclient, table_path, df):\n",
    "    \"\"\"\n",
    "    insert_existing_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Compares the new data to the existing data in the table.\n",
    "    Inserts the new/modified records to the existing table\n",
    "    \"\"\"\n",
    "    upload_bigquery_table( bqclient, table_path, \"WRITE_APPEND\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
    "    \"\"\"\n",
    "    query_bigquery_table\n",
    "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
    "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
    "    Returns the dataframe\n",
    "    \"\"\"    \n",
    "    bq_df = pd.DataFrame\n",
    "    # sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
    "    sql_query = 'SELECT * FROM `' + table_path + '`'\n",
    "    bq_df = bqclient.query(sql_query).to_dataframe()\n",
    "    return bq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_lookup(dimension_name, lookup_columns, df):\n",
    "    \"\"\"\n",
    "    dimension_lookup\n",
    "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
    "    Returns dataframe augmented with the surrogate keys\n",
    "    \"\"\"\n",
    "    bq_df = pd.DataFrame\n",
    "    surrogate_key = dimension_name+\"_dim_id\"\n",
    "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
    "    # Fetch the existing table\n",
    "    bq_df = query_bigquery_table(dimension_table_path, bqclient, surrogate_key)\n",
    "    if dimension_name == 'date':\n",
    "        bq_df['full_date'] = bq_df['full_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "    print(bq_df)\n",
    "    # Melt the dimension dataframe into an index with the lookup columns\n",
    "    m = bq_df.melt(id_vars=lookup_columns, value_vars=surrogate_key)\n",
    "    print(m)\n",
    "    # Rename the \"value\" column to the surrogate key column name\n",
    "    m=m.rename(columns={\"value\":surrogate_key})\n",
    "    # Merge with the fact table record\n",
    "    df = df.merge(m, on=lookup_columns, how='left')\n",
    "    # Drop the \"variable\" column and the lookup columns\n",
    "    df = df.drop(columns=lookup_columns)\n",
    "    df = df.drop(columns=\"variable\")\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df, bq_dataset, dimension_name):\n",
    "\n",
    "    # Renaming for 311\n",
    "    if bq_dataset == '311_illegal_parking':\n",
    "        if dimension_name == 'complaint':\n",
    "            df = df.rename(columns={'descriptor': 'complaint_description'})\n",
    "        elif dimension_name == 'complaint_source':\n",
    "            df = df.rename(columns={'open_data_channel_type': 'complaint_source_channel'})\n",
    "        elif dimension_name == 'location':\n",
    "            df = df.rename(columns={'city': 'incident_city', 'incident_zip': 'incident_zipcode'})\n",
    "        elif dimension_name == 'date':\n",
    "            df = df.rename(columns={'created_date': 'full_date'})\n",
    "\n",
    "    # Renaming for Open Parking\n",
    "    elif bq_dataset == 'open_parking':\n",
    "        if dimension_name == 'agency':\n",
    "            df = df.rename(columns={'issuing_agency': 'agency_name'})\n",
    "        elif dimension_name == 'location':\n",
    "            df = df.rename(columns={'precinct': 'precinct_num', 'county': 'borough'})\n",
    "        elif dimension_name == 'violation':\n",
    "            df = df.rename(columns={'violation': 'violation_description'})\n",
    "        elif dimension_name == 'violator':\n",
    "            df = df.rename(columns={'plate': 'violator_plate', 'state': 'violator_state'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null_values(df, bq_dataset, dimension_name):\n",
    "    # Renaming for 311\n",
    "    if bq_dataset == '311_illegal_parking':\n",
    "      if dimension_name == 'location':\n",
    "        default_values = {\n",
    "          'city': 'Unspecified',\n",
    "          'incident_zip': 0,\n",
    "          'borough': 'Unspecified'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "        \n",
    "    # Renaming for Open Parking\n",
    "    elif bq_dataset == 'open_parking':\n",
    "      if dimension_name == 'agency':\n",
    "        default_values = {\n",
    "          'issuing_agency': 'N/A'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "\n",
    "      elif dimension_name == 'violation':\n",
    "        default_values = {\n",
    "          'violation_status': 'N/A'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "\n",
    "      elif dimension_name == 'violator':\n",
    "        default_values = {\n",
    "          'plate': 'N/A'\n",
    "        }\n",
    "        df.fillna(default_values, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame\n",
    "    # Create the BigQuery Client\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    bq_dataset = '311_illegal_parking'\n",
    "    fact_name = '311_illegal_parking'\n",
    "    table_name = fact_name + '_fact'\n",
    "    # Construct the full BigQuery path to the table\n",
    "    fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
    "    file_source_path = 'data/311_master.csv'\n",
    "\n",
    "    # Load in the data file\n",
    "    with open(file_source_path, 'r') as data:\n",
    "            df = pd.read_csv(data)\n",
    "    # Set all of the column names to lower case letters\n",
    "    df = df.rename(columns=str.lower)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'complaint')\n",
    "    df = dimension_lookup(dimension_name='complaint', lookup_columns=['complaint_type', 'complaint_description'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'complaint_source') \n",
    "    df = dimension_lookup(dimension_name='complaint_source', lookup_columns=['complaint_source_channel'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'date') \n",
    "    # Convert column to datetime\n",
    "    df['full_date'] = pd.to_datetime(df['full_date'])\n",
    "    df['year'] = df['full_date'].dt.year\n",
    "    df['month'] = df['full_date'].dt.month\n",
    "    df['month_name'] = df['full_date'].dt.strftime('%B')\n",
    "    df['day'] = df['full_date'].dt.day\n",
    "    df['weekday_name'] = df['full_date'].dt.strftime('%A')\n",
    "    df['full_date'] = df['full_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    df = dimension_lookup(dimension_name='date', lookup_columns=['full_date', 'year', 'month', 'month_name', 'day', 'weekday_name'], df=df)\n",
    "\n",
    "    df = handle_null_values(df, bq_dataset, 'location')\n",
    "    df = rename_column(df, bq_dataset, 'location') \n",
    "    df = dimension_lookup(dimension_name='location', lookup_columns=['borough', 'incident_city', 'incident_zipcode'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'status') \n",
    "    df = dimension_lookup(dimension_name='status', lookup_columns=['status'], df=df)\n",
    "\n",
    "    # A list of all of the surrogate keys\n",
    "    # For transaction grain, also include the 'unique_key' column\n",
    "    surrogate_keys=['unique_key', 'complaint_dim_id','complaint_source_dim_id','date_dim_id','location_dim_id','status_dim_id']\n",
    "    \n",
    "    # Remove all of the other non-surrogate key columns\n",
    "    df = df[surrogate_keys]\n",
    "\n",
    "    # See if the target table exists\n",
    "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
    "    # If the target table does not exist, load all of the data into a new table\n",
    "    if not target_table_exists:\n",
    "        build_new_table( bqclient, fact_table_path, df)\n",
    "    # If the target table exists, then perform an incremental load\n",
    "    if target_table_exists:\n",
    "        insert_existing_table( bqclient, fact_table_path, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN PARKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Not found: Table cis-4400-404715:311_illegal_parking.agency_dimension was not found in location US\n\nLocation: US\nJob ID: cc8f6a47-2292-4afb-b55b-ce577cd5d5df\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\create_fact_table.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mlower)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m df \u001b[39m=\u001b[39m rename_column(df, bq_dataset, \u001b[39m'\u001b[39m\u001b[39magency\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m df \u001b[39m=\u001b[39m dimension_lookup(dimension_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39magency\u001b[39;49m\u001b[39m'\u001b[39;49m, lookup_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39magency_name\u001b[39;49m\u001b[39m'\u001b[39;49m], df\u001b[39m=\u001b[39;49mdf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df \u001b[39m=\u001b[39m rename_column(df, bq_dataset, \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Convert column to datetime\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\create_fact_table.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dimension_table_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([gcp_project,bq_dataset,dimension_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_dimension\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Fetch the existing table\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m bq_df \u001b[39m=\u001b[39m query_bigquery_table(dimension_table_path, bqclient, surrogate_key)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m dimension_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     bq_df[\u001b[39m'\u001b[39m\u001b[39mfull_date\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m bq_df[\u001b[39m'\u001b[39m\u001b[39mfull_date\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\create_fact_table.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m sql_query \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSELECT * FROM `\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m table_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m bq_df \u001b[39m=\u001b[39m bqclient\u001b[39m.\u001b[39;49mquery(sql_query)\u001b[39m.\u001b[39;49mto_dataframe()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andyy/Desktop/cis-4400/create_fact_table.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m bq_df\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1859\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype)\u001b[0m\n\u001b[0;32m   1693\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_dataframe\u001b[39m(\n\u001b[0;32m   1694\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1695\u001b[0m     bqstorage_client: Optional[\u001b[39m\"\u001b[39m\u001b[39mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1708\u001b[0m     timestamp_dtype: Union[Any, \u001b[39mNone\u001b[39;00m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1709\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpandas.DataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1710\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \n\u001b[0;32m   1712\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m \u001b[39m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1859\u001b[0m     query_result \u001b[39m=\u001b[39m wait_for_query(\u001b[39mself\u001b[39;49m, progress_bar_type, max_results\u001b[39m=\u001b[39;49mmax_results)\n\u001b[0;32m   1860\u001b[0m     \u001b[39mreturn\u001b[39;00m query_result\u001b[39m.\u001b[39mto_dataframe(\n\u001b[0;32m   1861\u001b[0m         bqstorage_client\u001b[39m=\u001b[39mbqstorage_client,\n\u001b[0;32m   1862\u001b[0m         dtypes\u001b[39m=\u001b[39mdtypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1873\u001b[0m         timestamp_dtype\u001b[39m=\u001b[39mtimestamp_dtype,\n\u001b[0;32m   1874\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\cloud\\bigquery\\_tqdm_helpers.py:104\u001b[0m, in \u001b[0;36mwait_for_query\u001b[1;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[0;32m    100\u001b[0m progress_bar \u001b[39m=\u001b[39m get_progress_bar(\n\u001b[0;32m    101\u001b[0m     progress_bar_type, \u001b[39m\"\u001b[39m\u001b[39mQuery is running\u001b[39m\u001b[39m\"\u001b[39m, default_total, \u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m progress_bar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m query_job\u001b[39m.\u001b[39;49mresult(max_results\u001b[39m=\u001b[39;49mmax_results)\n\u001b[0;32m    106\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1580\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     \u001b[39mif\u001b[39;00m retry_do_query \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m job_retry \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1578\u001b[0m         do_get_result \u001b[39m=\u001b[39m job_retry(do_get_result)\n\u001b[1;32m-> 1580\u001b[0m     do_get_result()\n\u001b[0;32m   1582\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mGoogleAPICallError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m   1583\u001b[0m     exc\u001b[39m.\u001b[39mmessage \u001b[39m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1584\u001b[0m         message\u001b[39m=\u001b[39mexc\u001b[39m.\u001b[39mmessage, location\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation, job_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_id\n\u001b[0;32m   1585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\api_core\\retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    369\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    371\u001b[0m )\n\u001b[1;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    373\u001b[0m     target,\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    375\u001b[0m     sleep_generator,\n\u001b[0;32m    376\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    377\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    378\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\api_core\\retry.py:207\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[0;32m    206\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m         result \u001b[39m=\u001b[39m target()\n\u001b[0;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39misawaitable(result):\n\u001b[0;32m    209\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1570\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.do_get_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_do_query \u001b[39m=\u001b[39m retry_do_query\n\u001b[0;32m   1568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_job_retry \u001b[39m=\u001b[39m job_retry\n\u001b[1;32m-> 1570\u001b[0m \u001b[39msuper\u001b[39;49m(QueryJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1572\u001b[0m \u001b[39m# Since the job could already be \"done\" (e.g. got a finished job\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[39m# via client.get_job), the superclass call to done() might not\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[39m# set the self._query_results cache.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reload_query_results(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[1;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(_AsyncJob, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mresult(timeout\u001b[39m=\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andyy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\google\\api_core\\future\\polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 Not found: Table cis-4400-404715:311_illegal_parking.agency_dimension was not found in location US\n\nLocation: US\nJob ID: cc8f6a47-2292-4afb-b55b-ce577cd5d5df\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.DataFrame\n",
    "    # Create the BigQuery Client\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "    # Construct a BigQuery client object\n",
    "    bqclient = bigquery.Client()\n",
    "\n",
    "    bq_dataset = 'open_parking'\n",
    "    fact_name = 'open_parking'\n",
    "    table_name = fact_name + '_fact'\n",
    "    # Construct the full BigQuery path to the table\n",
    "    fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
    "    file_source_path = 'data/open_parking_master.csv'\n",
    "\n",
    "    # Load in the data file\n",
    "    with open(file_source_path, 'r') as data:\n",
    "            df = pd.read_csv(data)\n",
    "    # Set all of the column names to lower case letters\n",
    "    df = df.rename(columns=str.lower)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'agency')\n",
    "    df = dimension_lookup(dimension_name='agency', lookup_columns=['agency_name'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'date') \n",
    "    # Convert column to datetime\n",
    "    df['full_date'] = pd.to_datetime(df['full_date'])\n",
    "    df['year'] = df['full_date'].dt.year\n",
    "    df['month'] = df['full_date'].dt.month\n",
    "    df['month_name'] = df['full_date'].dt.strftime('%B')\n",
    "    df['day'] = df['full_date'].dt.day\n",
    "    df['weekday_name'] = df['full_date'].dt.strftime('%A')\n",
    "    df['full_date'] = df['full_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    df = dimension_lookup(dimension_name='date', lookup_columns=['full_date', 'year', 'month', 'month_name', 'day', 'weekday_name'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'location')\n",
    "    df = dimension_lookup(dimension_name='location', lookup_columns=['precinct_num', 'borough', 'incident_zipcode'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'violation') \n",
    "    df = dimension_lookup(dimension_name='violation', lookup_columns=['violation_description', 'violation_status'], df=df)\n",
    "\n",
    "    df = rename_column(df, bq_dataset, 'violator') \n",
    "    df = dimension_lookup(dimension_name='violator', lookup_columns=['violator_plate', 'violator_state', 'license_type'], df=df)\n",
    "\n",
    "    # A list of all of the surrogate keys\n",
    "    # For transaction grain, also include the 'unique_key' column\n",
    "    surrogate_keys=['unique_key', 'agency_dim_id', 'location_dim_id', 'date_dim_id', 'violation_dim_id', 'violator_dim_id']\n",
    "    \n",
    "    # Remove all of the other non-surrogate key columns\n",
    "    df = df[surrogate_keys]\n",
    "\n",
    "    # See if the target table exists\n",
    "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
    "    # If the target table does not exist, load all of the data into a new table\n",
    "    if not target_table_exists:\n",
    "        build_new_table( bqclient, fact_table_path, df)\n",
    "    # If the target table exists, then perform an incremental load\n",
    "    if target_table_exists:\n",
    "        insert_existing_table( bqclient, fact_table_path, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
