{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the native Google BigQuery API module:\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a service account key file, save the path to that file in credentials.py and import credentials\n",
    "path_to_service_account_key_file = \"keys.json\"\n",
    "#!pip install credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "311 dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the name of the dimension\n",
    "# dimension_name = 'location'\n",
    "\n",
    "# # Set the name of the surrogate key\n",
    "# surrogate_key = f\"{dimension_name}_dim_id\"\n",
    "\n",
    "# # # Set the name of the business key\n",
    "# # business_key = f'{dimension_name}_id'\n",
    "\n",
    "# # Set the GCP Project, dataset and table name\n",
    "# gcp_project = 'cis-4400-404715'\n",
    "# bq_dataset = '311_illegal_parking'\n",
    "# table_name = f\"{dimension_name}_dimension\"\n",
    "# # Construct the full BigQuery path to the table\n",
    "# dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
    "\n",
    "# # Set the path to the source data files. Use double-slash for Windows paths C:\\\\myfolder\n",
    "# # For Linux use forward slashes    /home/username/python_etl\n",
    "# # For Mac use forward slashes      /users/username/python_etl\n",
    "# # file_source_path = 'c:\\\\Python_ETL'\n",
    "# # file_source_path = 'C:\\\\Users\\\\rholo\\\\OneDrive\\\\Documents\\\\classes\\\\4400\\\\311'\n",
    "# file_source_path = 'C:\\\\Users\\\\andyy\\\\Desktop\\\\cis-4400\\\\data\\\\311_master.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df: pd.DataFrame, column_list):\n",
    "    \"\"\"\n",
    "    transform_data\n",
    "    Accepts a data frame\n",
    "    Performs any specific cleaning and transformation steps on the dataframe\n",
    "    Returns the modified dataframe\n",
    "    This function can be modified based on required changes\n",
    "    \"\"\"\n",
    "    # Select the columns for this dimension\n",
    "    df = df[column_list]\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_client():\n",
    "    \"\"\"\n",
    "    create_bigquery_client\n",
    "    Creates a BigQuery client using the path to the service account key file\n",
    "    for credentials.\n",
    "    Returns the BigQuery client object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If authenticating using a service account key file, use the following code:\n",
    "        bqclient = bigquery.Client.from_service_account_json(service_account_key)\n",
    "        # Google Colab authentication already completed\n",
    "        bqclient = bigquery.Client(gcp_project)\n",
    "        return bqclient\n",
    "    except Exception as err:\n",
    "        print(\"error\")\n",
    "        # os._exit(-1)\n",
    "    return bqclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
    "    \"\"\"\n",
    "    upload_bigquery_table\n",
    "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
    "    Loads the data into the BigQuery table from the dataframe.\n",
    "    for credentials.\n",
    "    The write disposition is either\n",
    "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
    "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up a BigQuery job configuration with the write_disposition.\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
    "        \n",
    "        # Submit the job\n",
    "        print(type(bqclient))\n",
    "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
    "        # Show the job results\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        #os._exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigquery_table_exists(bqclient, table_path):\n",
    "    \"\"\"\n",
    "    bigquery_table_exists\n",
    "    Accepts a path to a BigQuery table\n",
    "    Checks if the BigQuery table exists.\n",
    "    Returns True or False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bqclient.get_table(table_path)  # Make an API request.\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
    "    \"\"\"\n",
    "    query_bigquery_table\n",
    "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
    "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
    "    Returns the dataframe\n",
    "    \"\"\"\n",
    "    bq_df = pd.DataFrame\n",
    "    sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
    "    try:\n",
    "        bq_df = bqclient.query(sql_query).to_dataframe()\n",
    "    except Exception as err:\n",
    "        print(\"error\")\n",
    "    return bq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surrogate_key(df, dimension_name='customers', offset=1):\n",
    "    \"\"\"\n",
    "    add_surrogate_key\n",
    "    Accepts a data frame and inserts an integer identifier as the first column\n",
    "    Returns the modified dataframe\n",
    "    \"\"\"\n",
    "    # Reset the index to count from 0\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # Add the new surrogate key starting from offset\n",
    "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_table(bqclient, dimension_table_path, dimension_name, df):\n",
    "    \"\"\"\n",
    "    build_new_table\n",
    "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
    "    Add the surrogate key and a record timestamp to the data frame\n",
    "    Inserts the contents of the dataframe to the dimensional table.\n",
    "    \"\"\"\n",
    "    # Add a surrogate key\n",
    "    df = add_surrogate_key(df, dimension_name, 1)\n",
    "    # Add the update timestamp\n",
    "    # Upload the dataframe to the BigQuery table\n",
    "    upload_bigquery_table(bqclient, dimension_table_path, \"WRITE_TRUNCATE\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df, bq_dataset, dimension_name):\n",
    "\n",
    "    # Renaming for 311\n",
    "    if bq_dataset == '311_illegal_parking':\n",
    "        if dimension_name == 'complaint':\n",
    "            df = df.rename(columns={'descriptor': 'complaint_description'})\n",
    "        elif dimension_name == 'complaint_source':\n",
    "            df = df.rename(columns={'open_data_channel_type': 'complaint_source_channel'})\n",
    "        elif dimension_name == 'location':\n",
    "            df = df.rename(columns={'city': 'incident_city', 'incident_zip': 'incident_zipcode'})\n",
    "\n",
    "    # Renaming for Open Parking\n",
    "    elif bq_dataset == 'open_parking':\n",
    "        if dimension_name == 'agency':\n",
    "            df = df.rename(columns={'issuing_agency': 'agency_name'})\n",
    "        elif dimension_name == 'location':\n",
    "            # NOTE: county is from API, and is not yet calculated.\n",
    "            # make sure to calculate county BEFORE calling this function\n",
    "            df = df.rename(columns={'precinct': 'precinct_num', 'county': 'borough'})\n",
    "        elif dimension_name == 'violation':\n",
    "            df = df.rename(columns={'violation': 'violation_description'})\n",
    "        elif dimension_name == 'violator':\n",
    "            df = df.rename(columns={'plate': 'violator_plate', 'state': 'violator_state'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_location_attributes(df):\n",
    "    precinct_to_zipcode = {\n",
    "        \"1\": \"10013\",\n",
    "        \"5\": \"10013\",\n",
    "        \"6\": \"10014\",\n",
    "        \"7\": \"10002\",\n",
    "        \"9\": \"10003\",\n",
    "        \"10\": \"10011\",\n",
    "        \"13\": \"10010\",\n",
    "        \"Midtown South\": \"10001\",\n",
    "        \"17\": \"10022\",\n",
    "        \"Midtown North\": \"10019\",\n",
    "        \"19\": \"10065\",\n",
    "        \"20\": \"10024\",\n",
    "        \"Central Park\": \"10024\",\n",
    "        \"23\": \"10029\",\n",
    "        \"24\": \"10025\",\n",
    "        \"25\": \"10035\",\n",
    "        \"26\": \"10027\",\n",
    "        \"28\": \"10027\",\n",
    "        \"30\": \"10031\",\n",
    "        \"32\": \"10030\",\n",
    "        \"33\": \"10032\",\n",
    "        \"34\": \"10033\",\n",
    "        \"40\": \"10454\",\n",
    "        \"41\": \"10459\",\n",
    "        \"42\": \"10451\",\n",
    "        \"43\": \"10473\",\n",
    "        \"44\": \"10452\",\n",
    "        \"45\": \"10465\",\n",
    "        \"46\": \"10457\",\n",
    "        \"47\": \"10466\",\n",
    "        \"48\": \"10457\",\n",
    "        \"49\": \"10461\",\n",
    "        \"50\": \"10463\",\n",
    "        \"52\": \"10467\",\n",
    "        \"60\": \"11224\",\n",
    "        \"61\": \"11223\",\n",
    "        \"62\": \"11214\",\n",
    "        \"63\": \"11210\",\n",
    "        \"66\": \"11204\",\n",
    "        \"67\": \"11226\",\n",
    "        \"68\": \"11220\",\n",
    "        \"69\": \"11236\",\n",
    "        \"70\": \"11230\",\n",
    "        \"71\": \"11225\",\n",
    "        \"72\": \"11232\",\n",
    "        \"73\": \"11212\",\n",
    "        \"75\": \"11208\",\n",
    "        \"76\": \"11231\",\n",
    "        \"77\": \"11213\",\n",
    "        \"78\": \"11217\",\n",
    "        \"79\": \"11216\",\n",
    "        \"81\": \"11221\",\n",
    "        \"83\": \"11237\",\n",
    "        \"84\": \"11201\",\n",
    "        \"88\": \"11205\",\n",
    "        \"90\": \"11211\",\n",
    "        \"94\": \"11222\",\n",
    "        \"100\": \"11693\",\n",
    "        \"101\": \"11691\",\n",
    "        \"102\": \"11418\",\n",
    "        \"103\": \"11432\",\n",
    "        \"104\": \"11385\",\n",
    "        \"105\": \"11428\",\n",
    "        \"106\": \"11417\",\n",
    "        \"107\": \"11365\",\n",
    "        \"108\": \"11101\",\n",
    "        \"109\": \"11354\",\n",
    "        \"110\": \"11373\",\n",
    "        \"111\": \"11361\",\n",
    "        \"112\": \"11375\",\n",
    "        \"113\": \"11434\",\n",
    "        \"114\": \"11103\",\n",
    "        \"115\": \"11372\",\n",
    "        \"120\": \"10301\",\n",
    "        \"121\": \"10314\",\n",
    "        \"122\": \"10306\",\n",
    "        \"123\": \"10307\"\n",
    "    }\n",
    "\n",
    "    zipcode_to_borough = {\n",
    "        \"10013\": \"Manhattan\",\n",
    "        \"10014\": \"Manhattan\",\n",
    "        \"10002\": \"Manhattan\",\n",
    "        \"10003\": \"Manhattan\",\n",
    "        \"10011\": \"Manhattan\",\n",
    "        \"10010\": \"Manhattan\",\n",
    "        \"10001\": \"Manhattan\",\n",
    "        \"10022\": \"Manhattan\",\n",
    "        \"10019\": \"Manhattan\",\n",
    "        \"10065\": \"Manhattan\",\n",
    "        \"10024\": \"Manhattan\",\n",
    "        \"10029\": \"Manhattan\",\n",
    "        \"10025\": \"Manhattan\",\n",
    "        \"10035\": \"Manhattan\",\n",
    "        \"10027\": \"Manhattan\",\n",
    "        \"10031\": \"Manhattan\",\n",
    "        \"10030\": \"Manhattan\",\n",
    "        \"10032\": \"Manhattan\",\n",
    "        \"10033\": \"Manhattan\",\n",
    "        \"10454\": \"Bronx\",\n",
    "        \"10459\": \"Bronx\",\n",
    "        \"10451\": \"Bronx\",\n",
    "        \"10473\": \"Bronx\",\n",
    "        \"10452\": \"Bronx\",\n",
    "        \"10465\": \"Bronx\",\n",
    "        \"10457\": \"Bronx\",\n",
    "        \"10466\": \"Bronx\",\n",
    "        \"10461\": \"Bronx\",\n",
    "        \"10463\": \"Bronx\",\n",
    "        \"10467\": \"Bronx\",\n",
    "        \"11224\": \"Brooklyn\",\n",
    "        \"11223\": \"Brooklyn\",\n",
    "        \"11214\": \"Brooklyn\",\n",
    "        \"11210\": \"Brooklyn\",\n",
    "        \"11204\": \"Brooklyn\",\n",
    "        \"11226\": \"Brooklyn\",\n",
    "        \"11220\": \"Brooklyn\",\n",
    "        \"11236\": \"Brooklyn\",\n",
    "        \"11230\": \"Brooklyn\",\n",
    "        \"11225\": \"Brooklyn\",\n",
    "        \"11232\": \"Brooklyn\",\n",
    "        \"11212\": \"Brooklyn\",\n",
    "        \"11208\": \"Brooklyn\",\n",
    "        \"11231\": \"Brooklyn\",\n",
    "        \"11213\": \"Brooklyn\",\n",
    "        \"11217\": \"Brooklyn\",\n",
    "        \"11216\": \"Brooklyn\",\n",
    "        \"11221\": \"Brooklyn\",\n",
    "        \"11237\": \"Brooklyn\",\n",
    "        \"11201\": \"Brooklyn\",\n",
    "        \"11205\": \"Brooklyn\",\n",
    "        \"11211\": \"Brooklyn\",\n",
    "        \"11222\": \"Brooklyn\",\n",
    "        \"11693\": \"Queens\",\n",
    "        \"11691\": \"Queens\",\n",
    "        \"11418\": \"Queens\",\n",
    "        \"11432\": \"Queens\",\n",
    "        \"11385\": \"Queens\",\n",
    "        \"11428\": \"Queens\",\n",
    "        \"11417\": \"Queens\",\n",
    "        \"11365\": \"Queens\",\n",
    "        \"11101\": \"Queens\",\n",
    "        \"11354\": \"Queens\",\n",
    "        \"11373\": \"Queens\",\n",
    "        \"11361\": \"Queens\",\n",
    "        \"11375\": \"Queens\",\n",
    "        \"11434\": \"Queens\",\n",
    "        \"11103\": \"Queens\",\n",
    "        \"11372\": \"Queens\",\n",
    "        \"10301\": \"Staten Island\",\n",
    "        \"10314\": \"Staten Island\",\n",
    "        \"10306\": \"Staten Island\",\n",
    "        \"10307\": \"Staten Island\"\n",
    "    }\n",
    "\n",
    "    df['zipcode'] = df['precinct'].map(precinct_to_zipcode)\n",
    "    df['borough'] = df['zipcode'].map(zipcode_to_borough)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\andyy\\\\Desktop\\\\cis-4400\\\\data\\\\311_master.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Andy\\Desktop\\cis-4400\\main.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/Desktop/cis-4400/main.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/Desktop/cis-4400/main.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Load in the data file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Andy/Desktop/cis-4400/main.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_source_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/Desktop/cis-4400/main.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Andy/Desktop/cis-4400/main.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# Set all of the column names to lower case letters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andy\\Desktop\\cis-4400\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\andyy\\\\Desktop\\\\cis-4400\\\\data\\\\311_master.csv'"
     ]
    }
   ],
   "source": [
    "# 311 311 311 311 311 311 311 311 311 311 311 311 311 311\n",
    "# Program main\n",
    "# Load the CSV File into a dataframe\n",
    "# Transform the Dataframe\n",
    "# Create a BigQuery client\n",
    "# See if the target dimension table exists\n",
    "#    If not exists, load the data into a new table\n",
    "#    If exists, insert new records into the table\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dim_dict = {\n",
    "    'location': ['borough', 'city', 'incident_zip', 'incident_address', 'location_type',],\n",
    "    'complaint': ['complaint_type', 'descriptor'],\n",
    "    'complaint_source': ['open_data_channel_type'],\n",
    "    'status': ['status'],\n",
    "    'date': ['created_date'],\n",
    "    }\n",
    "\n",
    "    for key, value in dim_dict.items():\n",
    "        dimension_name = key\n",
    "        column = value\n",
    "\n",
    "        # Set the name of the surrogate key\n",
    "        surrogate_key = f\"{dimension_name}_dim_id\"\n",
    "\n",
    "        # Set the GCP Project, dataset and table name\n",
    "        gcp_project = 'cis-4400-404715'\n",
    "        bq_dataset = '311_illegal_parking'\n",
    "        table_name = f\"{dimension_name}_dimension\"\n",
    "        # Construct the full BigQuery path to the table\n",
    "        dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
    "\n",
    "        # Set the path to the source data files. Use double-slash for Windows paths C:\\\\myfolder\n",
    "        file_source_path = 'C:\\\\Users\\\\andyy\\\\Desktop\\\\cis-4400\\\\data\\\\311_master.csv'\n",
    "\n",
    "        df = pd.DataFrame\n",
    "        # Load in the data file\n",
    "        with open(file_source_path, 'r') as data:\n",
    "                df = pd.read_csv(data)\n",
    "            # Set all of the column names to lower case letters\n",
    "        df = df.rename(columns=str.lower)\n",
    "            \n",
    "            \n",
    "        #df = load_csv_data_file(file_source_path, \"my_311_data_WaterQuality.csv\", df)\n",
    "        # Transform the data\n",
    "        df = transform_data(df, column)\n",
    "    \n",
    "        df = rename_column(df, bq_dataset, dimension_name)\n",
    "    \n",
    "        if dimension_name == 'date':\n",
    "            df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "            df = pd.DataFrame({\n",
    "                'full_date': df['created_date'].dt.date,\n",
    "                'year': df['created_date'].dt.year,\n",
    "                'month': df['created_date'].dt.month,\n",
    "                'month_name': df['created_date'].dt.strftime('%B'),  # Month name\n",
    "                'day': df['created_date'].dt.day,\n",
    "                'weekday_name': df['created_date'].dt.strftime('%A'),  # Month name\n",
    "            })\n",
    "            \n",
    "        # Create the BigQuery Client\n",
    "        # setup enviroment parameters to connect to BQ project\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "        # Construct a BigQuery client object\n",
    "        bqclient = bigquery.Client()\n",
    "\n",
    "        # See if the target dimensional table exists\n",
    "        target_table_exists = bigquery_table_exists(bqclient, dimension_table_path  )\n",
    "\n",
    "        # If the target dimension table does not exist, load all of the data into a new table\n",
    "        if not target_table_exists:\n",
    "            build_new_table( bqclient, dimension_table_path, dimension_name, df)\n",
    "        # If the target table exists, then perform an incremental load\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN PARKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'google.cloud.bigquery.client.Client'>\n"
     ]
    }
   ],
   "source": [
    "# OPEN PARKING OPEN PARKING OPEN PARKING OPEN PARKING OPEN PARKING OPEN PARKING OPEN PARKING\n",
    "# Program main\n",
    "# Load the CSV File into a dataframe\n",
    "# Transform the Dataframe\n",
    "# Create a BigQuery client\n",
    "# See if the target dimension table exists\n",
    "#    If not exists, load the data into a new table\n",
    "#    If exists, insert new records into the table\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dim_dict = {\n",
    "      'location': ['precinct'],\n",
    "      'agency': ['issuing_agency'],\n",
    "      'violation': ['violation', 'violation_status'],\n",
    "      'violator': ['plate', 'state', 'license_type'],\n",
    "      'date': ['issue_date',]\n",
    "    }\n",
    "\n",
    "    for key, value in dim_dict.items():\n",
    "        dimension_name = key\n",
    "        column = value\n",
    "\n",
    "        # Set the name of the surrogate key\n",
    "        surrogate_key = f\"{dimension_name}_dim_id\"\n",
    "\n",
    "        # Set the GCP Project, dataset and table name\n",
    "        gcp_project = 'cis-4400-404715'\n",
    "        bq_dataset = 'open_parking'\n",
    "        table_name = f\"{dimension_name}_dimension\"\n",
    "        # Construct the full BigQuery path to the table\n",
    "        dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
    "\n",
    "        # Set the path to the source data files. Use double-slash for Windows paths C:\\\\myfolder\n",
    "        file_source_path = 'C:\\\\Users\\\\andyy\\\\Desktop\\\\cis-4400\\\\data\\\\open_parking_master.csv'\n",
    "\n",
    "        df = pd.DataFrame\n",
    "        # Load in the data file\n",
    "        with open(file_source_path, 'r') as data:\n",
    "                df = pd.read_csv(data)\n",
    "            # Set all of the column names to lower case letters\n",
    "        df = df.rename(columns=str.lower)\n",
    "            \n",
    "            \n",
    "        #df = load_csv_data_file(file_source_path, \"my_311_data_WaterQuality.csv\", df)\n",
    "        # Transform the data\n",
    "        df = transform_data(df, column)\n",
    "\n",
    "        # Call a function here that calculates the zipcode based on the precinct number\n",
    "# --> --> -->\n",
    "        if dimension_name == 'location':\n",
    "            df = calculate_location_attributes(df)\n",
    "\n",
    "\n",
    "        df = rename_column(df, bq_dataset, dimension_name)\n",
    "\n",
    "        if dimension_name == 'date':\n",
    "            # Convert 'issue_date' to datetime using the specified format\n",
    "            df['issue_date'] = pd.to_datetime(df['issue_date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "            # Create a DataFrame with extracted date components\n",
    "            df = pd.DataFrame({\n",
    "                'full_date': df['issue_date'].dt.date,\n",
    "                'year': df['issue_date'].dt.year,\n",
    "                'month': df['issue_date'].dt.month,\n",
    "                'month_name': df['issue_date'].dt.strftime('%B'),  # Month name\n",
    "                'day': df['issue_date'].dt.day,\n",
    "                'weekday_name': df['issue_date'].dt.strftime('%A'),  # Weekday name\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "        # Create the BigQuery Client\n",
    "        # setup enviroment parameters to connect to BQ project\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
    "\n",
    "        # Construct a BigQuery client object\n",
    "        bqclient = bigquery.Client()\n",
    "\n",
    "        # See if the target dimensional table exists\n",
    "        target_table_exists = bigquery_table_exists(bqclient, dimension_table_path  )\n",
    "\n",
    "        # If the target dimension table does not exist, load all of the data into a new table\n",
    "        if not target_table_exists:\n",
    "            build_new_table( bqclient, dimension_table_path, dimension_name, df)\n",
    "        # If the target table exists, then perform an incremental load\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
